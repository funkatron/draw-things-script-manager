#!/usr/bin/env python3
"""
Draw Things Script Manager

A tool to help manage scripts in Draw Things:
- List all scripts with metadata
- Validate scripts (check for missing files, invalid JSON)
- Find orphaned entries
- Sync metadata between files and JSON
- Extract metadata from script comments

SAFETY: This tool ONLY modifies custom_scripts.json. It NEVER modifies,
renames, or deletes script files themselves. All operations are idempotent.

FILE OPERATIONS:
- install: Creates new files (can use custom name), never renames existing files
- sync: Only adds entries to JSON, never touches actual script files
- All other commands: Read-only operations on script files
"""

import json
import os
import sys
import subprocess
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict


@dataclass
class ScriptMetadata:
    """Represents script metadata from custom_scripts.json"""
    name: str
    file: str
    description: Optional[str] = None
    author: Optional[str] = None
    tags: Optional[List[str]] = None
    images: Optional[List[Dict]] = None
    base_color: Optional[str] = None
    favicon: Optional[str] = None
    file_path: Optional[str] = None
    type: Optional[str] = None
    is_sample_duplicate: Optional[bool] = None

    @classmethod
    def from_dict(cls, data: Dict) -> 'ScriptMetadata':
        """Create from dictionary (handles snake_case conversion)"""
        # Convert camelCase to snake_case for some fields
        field_map = {
            'baseColor': 'base_color',
            'filePath': 'file_path',
            'isSampleDuplicate': 'is_sample_duplicate',
        }

        converted = {}
        for key, value in data.items():
            snake_key = field_map.get(key, key)
            converted[snake_key] = value

        return cls(**converted)


class ScriptManager:
    def __init__(self, scripts_dir: Optional[str] = None, use_git: bool = True):
        if scripts_dir is None:
            # Default to the Draw Things Scripts directory
            scripts_dir = Path.home() / "Library/Containers/com.liuliu.draw-things/Data/Documents/Scripts"

        self.scripts_dir = Path(scripts_dir)
        self.custom_scripts_json = self.scripts_dir / "custom_scripts.json"
        self.use_git = use_git and shutil.which('git') is not None
        self._ensure_git_repo()

    def load_metadata(self) -> List[ScriptMetadata]:
        """Load script metadata from custom_scripts.json"""
        if not self.custom_scripts_json.exists():
            return []

        try:
            with open(self.custom_scripts_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return [ScriptMetadata.from_dict(item) for item in data]
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON: {e}", file=sys.stderr)
            return []

    def get_script_files(self) -> List[Path]:
        """Get all .js files in the scripts directory"""
        if not self.scripts_dir.exists():
            return []

        return [f for f in self.scripts_dir.iterdir()
                if f.is_file() and f.suffix == '.js' and f.name != 'custom_scripts.json']

    def _ensure_git_repo(self) -> None:
        """Ensure the scripts directory is a git repository"""
        if not self.use_git:
            return

        git_dir = self.scripts_dir / '.git'
        if not git_dir.exists():
            try:
                self._run_git(['init'], check=True)
                # Create .gitignore if it doesn't exist
                gitignore = self.scripts_dir / '.gitignore'
                if not gitignore.exists():
                    with open(gitignore, 'w') as f:
                        f.write("# Auto-generated by script_manager.py\n")
                        f.write("*.sqlite3*\n")
                        f.write("*.shm\n")
                        f.write("*.wal\n")
                        f.write("script_summary.txt\n")
                self._run_git(['add', '.gitignore'], check=False)
                self._run_git(['commit', '-m', 'Initial commit: Add .gitignore'], check=False)
                print("‚ÑπÔ∏è  Initialized git repository in scripts directory", file=sys.stderr)
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Could not initialize git repo: {e}", file=sys.stderr)
                self.use_git = False

    def _run_git(self, args: List[str], check: bool = True, capture_output: bool = False) -> Optional[subprocess.CompletedProcess]:
        """Run a git command"""
        if not self.use_git:
            return None

        try:
            cmd = ['git', '-C', str(self.scripts_dir)] + args
            result = subprocess.run(
                cmd,
                check=check,
                capture_output=capture_output,
                text=True,
                timeout=30
            )
            return result
        except subprocess.CalledProcessError as e:
            if check:
                raise
            return None
        except Exception as e:
            if check:
                raise
            return None

    def _backup_file(self, file_path: Path) -> Optional[Path]:
        """Create a backup of a file before modification"""
        if not file_path.exists():
            return None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = file_path.parent / f"{file_path.name}.backup_{timestamp}"

        try:
            shutil.copy2(file_path, backup_path)
            return backup_path
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not create backup: {e}", file=sys.stderr)
            return None

    def _save_metadata(self, metadata_list: List[ScriptMetadata], commit_message: Optional[str] = None) -> bool:
        """Save metadata to JSON file with git integration"""
        # Validate JSON before writing
        data = []
        for m in metadata_list:
            d = asdict(m)
            # Remove None values
            d = {k: v for k, v in d.items() if v is not None}
            # Convert snake_case back to camelCase
            field_map = {
                'base_color': 'baseColor',
                'file_path': 'filePath',
                'is_sample_duplicate': 'isSampleDuplicate',
            }
            converted = {}
            for key, value in d.items():
                camel_key = field_map.get(key, key)
                converted[camel_key] = value
            data.append(converted)

        # Validate JSON can be serialized
        try:
            json_str = json.dumps(data, indent=2, ensure_ascii=False)
            json.loads(json_str)  # Validate it's valid JSON
        except (TypeError, ValueError) as e:
            print(f"‚ùå Error: Invalid data structure: {e}", file=sys.stderr)
            return False

        # Create backup
        backup_path = self._backup_file(self.custom_scripts_json)

        try:
            # Write to file
            with open(self.custom_scripts_json, 'w', encoding='utf-8') as f:
                f.write(json_str)

            # Stage and commit if using git
            if self.use_git and commit_message:
                self._run_git(['add', str(self.custom_scripts_json)], check=False)
                self._run_git(['commit', '-m', commit_message], check=False)

            if backup_path:
                # Clean up backup after successful write
                try:
                    backup_path.unlink()
                except:
                    pass

            return True
        except Exception as e:
            print(f"‚ùå Error writing file: {e}", file=sys.stderr)
            # Restore from backup if write failed
            if backup_path and backup_path.exists():
                try:
                    shutil.copy2(backup_path, self.custom_scripts_json)
                    print(f"‚úì Restored from backup: {backup_path}", file=sys.stderr)
                except:
                    pass
            return False

    def git_status(self) -> Dict[str, List[str]]:
        """Get git status of scripts directory"""
        if not self.use_git:
            return {'error': ['Git not available']}

        status = {
            'modified': [],
            'untracked': [],
            'staged': [],
            'clean': False
        }

        try:
            # Check if there are any changes
            result = self._run_git(['status', '--porcelain'], capture_output=True, check=False)
            if result and result.returncode == 0:
                lines = result.stdout.strip().split('\n') if result.stdout.strip() else []
                if not lines:
                    status['clean'] = True
                else:
                    for line in lines:
                        if line.startswith('??'):
                            status['untracked'].append(line[3:])
                        elif line.startswith(' M') or line.startswith('MM'):
                            status['modified'].append(line[3:])
                        elif line.startswith('M ') or line.startswith('A '):
                            status['staged'].append(line[3:])
        except Exception as e:
            status['error'] = [str(e)]

        return status

    def extract_metadata_from_script(self, script_path: Path) -> Dict:
        """Extract metadata from script file comments"""
        metadata = {}

        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # Look for comment-based metadata
            for line in lines[:20]:  # Check first 20 lines
                line = line.strip()
                if line.startswith('//'):
                    line = line[2:].strip()
                    if ':' in line:
                        key, value = line.split(':', 1)
                        key = key.strip().lower()
                        value = value.strip()

                        if key in ['author', 'name', 'description', 'version', 'v']:
                            if key == 'v' or key == 'version':
                                metadata['version'] = value
                            else:
                                metadata[key] = value
        except Exception as e:
            print(f"Error reading {script_path}: {e}", file=sys.stderr)

        return metadata

    def validate(self) -> Dict:
        """Validate scripts and return issues"""
        issues = {
            'missing_files': [],  # In JSON but file doesn't exist
            'orphaned_files': [],  # File exists but not in JSON
            'invalid_json': False,
            'duplicate_files': [],
            'case_mismatches': []  # Case differences between JSON and files
        }

        # Load metadata
        try:
            metadata_list = self.load_metadata()
        except Exception as e:
            issues['invalid_json'] = True
            issues['json_error'] = str(e)
            metadata_list = []

        # Get all script files
        script_files = self.get_script_files()
        file_names = {f.name for f in script_files}
        file_names_lower = {f.name.lower(): f.name for f in script_files}

        # Check for missing files and case mismatches
        metadata_files = {m.file for m in metadata_list}
        for metadata in metadata_list:
            if metadata.file not in file_names:
                # Check case-insensitive match
                if metadata.file.lower() in file_names_lower:
                    actual_file = file_names_lower[metadata.file.lower()]
                    issues['case_mismatches'].append((metadata.file, actual_file))
                else:
                    issues['missing_files'].append(metadata.file)

        # Check for orphaned files (case-insensitive)
        metadata_files_lower = {m.file.lower() for m in metadata_list}
        for script_file in script_files:
            if script_file.name.lower() not in metadata_files_lower:
                issues['orphaned_files'].append(script_file.name)

        # Check for duplicate file entries in JSON (case-insensitive)
        file_counts = defaultdict(int)
        for metadata in metadata_list:
            file_counts[metadata.file.lower()] += 1

        issues['duplicate_files'] = [f for f, count in file_counts.items() if count > 1]

        return issues

    def list_scripts(self, show_details: bool = False) -> None:
        """List all scripts"""
        metadata_list = self.load_metadata()
        script_files = self.get_script_files()

        # Create a mapping of file (case-insensitive) -> metadata
        metadata_map = {m.file.lower(): m for m in metadata_list}
        file_names_lower = {f.name.lower(): f.name for f in script_files}

        print(f"\n{'='*80}")
        print(f"Draw Things Scripts Manager")
        print(f"{'='*80}\n")
        print(f"Scripts Directory: {self.scripts_dir}")
        print(f"Total Script Files: {len(script_files)}")
        print(f"Scripts with Metadata: {len(metadata_list)}")
        print(f"\n{'='*80}\n")

        # List scripts with metadata
        if metadata_list:
            print("Scripts with Metadata:")
            print("-" * 80)
            for metadata in sorted(metadata_list, key=lambda x: x.name.lower()):
                print(f"\nüìú {metadata.name}")
                print(f"   File: {metadata.file}")
                if metadata.author:
                    print(f"   Author: {metadata.author}")
                if metadata.description:
                    print(f"   Description: {metadata.description[:100]}...")
                if metadata.tags:
                    print(f"   Tags: {', '.join(metadata.tags)}")

                # Check if file exists (case-insensitive)
                file_path = self.scripts_dir / metadata.file
                if file_path.exists():
                    size = file_path.stat().st_size
                    print(f"   Status: ‚úì File exists ({size:,} bytes)")
                elif metadata.file.lower() in file_names_lower:
                    actual_file = file_names_lower[metadata.file.lower()]
                    actual_path = self.scripts_dir / actual_file
                    size = actual_path.stat().st_size
                    print(f"   Status: ‚ö† Case mismatch (JSON: {metadata.file}, File: {actual_file}) ({size:,} bytes)")
                else:
                    print(f"   Status: ‚úó File missing!")

                if show_details:
                    if metadata.images:
                        print(f"   Images: {len(metadata.images)}")
                    if metadata.base_color:
                        print(f"   Base Color: {metadata.base_color}")

        # List orphaned files (case-insensitive check)
        orphaned = [f for f in script_files if f.name.lower() not in metadata_map]
        if orphaned:
            print(f"\n\nOrphaned Files (not in metadata):")
            print("-" * 80)
            for script_file in sorted(orphaned, key=lambda x: x.name.lower()):
                size = script_file.stat().st_size
                print(f"  ‚Ä¢ {script_file.name} ({size:,} bytes)")

                if show_details:
                    # Try to extract metadata from comments
                    extracted = self.extract_metadata_from_script(script_file)
                    if extracted:
                        print(f"    Extracted: {extracted}")

        print(f"\n{'='*80}\n")

    def validate_and_report(self) -> bool:
        """Validate scripts and print report"""
        issues = self.validate()

        print(f"\n{'='*80}")
        print("Validation Report")
        print(f"{'='*80}\n")

        has_issues = False

        if issues['invalid_json']:
            print("‚ùå Invalid JSON in custom_scripts.json")
            print(f"   Error: {issues.get('json_error', 'Unknown error')}")
            has_issues = True
        else:
            print("‚úì JSON is valid")

        if issues['missing_files']:
            print(f"\n‚ùå Missing Files ({len(issues['missing_files'])}):")
            for file in issues['missing_files']:
                print(f"   ‚Ä¢ {file}")
            has_issues = True
        else:
            print("\n‚úì All metadata files exist")

        if issues['orphaned_files']:
            print(f"\n‚ö†Ô∏è  Orphaned Files ({len(issues['orphaned_files'])}):")
            print("   (Files exist but not in metadata)")
            for file in issues['orphaned_files']:
                print(f"   ‚Ä¢ {file}")
        else:
            print("\n‚úì No orphaned files")

        if issues['duplicate_files']:
            print(f"\n‚ùå Duplicate Entries ({len(issues['duplicate_files'])}):")
            for file in issues['duplicate_files']:
                print(f"   ‚Ä¢ {file}")
            has_issues = True
        else:
            print("\n‚úì No duplicate entries")

        if issues['case_mismatches']:
            print(f"\n‚ö†Ô∏è  Case Mismatches ({len(issues['case_mismatches'])}):")
            print("   (JSON filename doesn't match actual file case)")
            for json_file, actual_file in issues['case_mismatches']:
                print(f"   ‚Ä¢ JSON: {json_file} ‚Üí File: {actual_file}")

        print(f"\n{'='*80}\n")

        return not has_issues

    def sync_metadata(self, dry_run: bool = True, files_to_manage: Optional[List[str]] = None) -> None:
        """
        Sync metadata - add orphaned files to JSON.

        This function ONLY modifies custom_scripts.json, never the script files themselves.
        It is idempotent - running it multiple times produces the same result.

        Args:
            dry_run: If True, only show what would be done
            files_to_manage: List of specific filenames to add. If None, adds all orphaned files.
        """
        issues = self.validate()

        if not issues['orphaned_files']:
            print("No orphaned files to sync.")
            return

        print(f"\n{'='*80}")
        print("Sync Metadata")
        print(f"{'='*80}\n")
        print("‚ö†Ô∏è  This will ONLY modify custom_scripts.json, never the script files themselves.\n")

        metadata_list = self.load_metadata()
        # Use case-insensitive matching for existing files
        existing_files_lower = {m.file.lower() for m in metadata_list}
        existing_files_map = {m.file.lower(): m.file for m in metadata_list}

        # Filter files to manage
        files_to_add = []
        if files_to_manage:
            # Only add explicitly requested files
            for file_name in files_to_manage:
                if file_name.lower() not in existing_files_lower:
                    if file_name in issues['orphaned_files']:
                        files_to_add.append(file_name)
                    else:
                        print(f"‚ö†Ô∏è  Skipping {file_name} (not found in orphaned files)")
                else:
                    actual_file = existing_files_map.get(file_name.lower(), file_name)
                    print(f"‚ÑπÔ∏è  Skipping {file_name} (already in metadata as {actual_file})")
        else:
            # Add all orphaned files
            files_to_add = issues['orphaned_files']

        if not files_to_add:
            print("No files to add.")
            return

        new_entries = []
        for file_name in files_to_add:
            script_path = self.scripts_dir / file_name
            if not script_path.exists():
                print(f"‚ö†Ô∏è  Skipping {file_name} (file does not exist)")
                continue

            # Check if already exists (case-insensitive) - idempotency check
            if file_name.lower() in existing_files_lower:
                actual_file = existing_files_map[file_name.lower()]
                print(f"‚ÑπÔ∏è  Skipping {file_name} (already exists as {actual_file} in metadata)")
                continue

            extracted = self.extract_metadata_from_script(script_path)

            # Create basic metadata entry
            name = file_name.replace('.js', '').replace('-', ' ').replace('_', ' ').title()
            entry = {
                'name': name,
                'file': file_name,  # Use actual filename as-is
                'description': extracted.get('description', ''),
                'author': extracted.get('author', ''),
            }

            new_entries.append(entry)
            print(f"{'Would add' if dry_run else 'Adding'}: {name} ({file_name})")

        if not new_entries:
            print("\nNo new entries to add.")
            return

        if not dry_run:
            # Add new entries to metadata (idempotent - won't add duplicates)
            for entry_dict in new_entries:
                entry = ScriptMetadata.from_dict(entry_dict)
                # Double-check it doesn't already exist
                if entry.file.lower() not in existing_files_lower:
                    metadata_list.append(entry)
                    existing_files_lower.add(entry.file.lower())

            # Save with git integration
            commit_msg = f"Add metadata for {len(new_entries)} script(s): {', '.join([e['file'] for e in new_entries])}"
            if self._save_metadata(metadata_list, commit_message=commit_msg):
                print(f"\n‚úì Added {len(new_entries)} entries to custom_scripts.json")
                if self.use_git:
                    print("‚úì Changes committed to git")
            else:
                print(f"\n‚ùå Failed to save metadata")
                return
        else:
            print(f"\n[DRY RUN] Would add {len(new_entries)} entries.")
            print("Use --no-dry-run to apply changes.")

        print(f"\n{'='*80}\n")

    def normalize(self, dry_run: bool = True, fix_case: bool = True,
                  normalize_json: bool = True, sort_entries: bool = True) -> None:
        """
        Normalize the scripts directory and metadata.

        This function normalizes metadata to match actual files and standardizes
        the JSON structure. It NEVER renames or moves actual script files.

        Args:
            dry_run: If True, only show what would be normalized
            fix_case: Fix case mismatches in metadata (update JSON to match file case)
            normalize_json: Normalize JSON structure (remove None values, standardize format)
            sort_entries: Sort entries alphabetically by name
        """
        print(f"\n{'='*80}")
        print("Normalize Scripts")
        print(f"{'='*80}\n")
        print("‚ö†Ô∏è  This will ONLY modify custom_scripts.json, never the script files themselves.\n")

        issues = self.validate()
        metadata_list = self.load_metadata()
        script_files = self.get_script_files()

        # Create mapping of actual files (case-insensitive)
        file_names_lower = {f.name.lower(): f.name for f in script_files}

        changes_made = []
        normalized_metadata = []

        # Fix case mismatches
        if fix_case and issues['case_mismatches']:
            print("Case Mismatches to Fix:")
            print("-" * 80)
            for json_file, actual_file in issues['case_mismatches']:
                print(f"   JSON: {json_file} ‚Üí File: {actual_file}")
                changes_made.append(f"Fix case: {json_file} ‚Üí {actual_file}")

        # Process each metadata entry
        for metadata in metadata_list:
            normalized = metadata

            # Fix case if needed
            if fix_case:
                # Check if file exists with different case
                if metadata.file.lower() in file_names_lower:
                    actual_file = file_names_lower[metadata.file.lower()]
                    if metadata.file != actual_file:
                        # Update to match actual file case
                        normalized = ScriptMetadata(
                            name=metadata.name,
                            file=actual_file,  # Use actual file case
                            description=metadata.description,
                            author=metadata.author,
                            tags=metadata.tags,
                            images=metadata.images,
                            base_color=metadata.base_color,
                            favicon=metadata.favicon,
                            file_path=metadata.file_path,
                            type=metadata.type,
                            is_sample_duplicate=metadata.is_sample_duplicate
                        )

            # Normalize JSON structure (remove None values, ensure consistent format)
            # This is handled when converting to dict for saving in _save_metadata

            normalized_metadata.append(normalized)

        # Sort entries if requested
        if sort_entries:
            normalized_metadata.sort(key=lambda x: x.name.lower())
            if normalized_metadata != metadata_list:
                changes_made.append("Sort entries alphabetically")

        # Check if any changes would be made
        if not changes_made and normalized_metadata == metadata_list:
            print("‚úì No normalization needed - everything is already normalized")
            print(f"\n{'='*80}\n")
            return

        # Show what would change
        if dry_run:
            print("\n[DRY RUN] Would make the following changes:")
            for change in changes_made:
                print(f"   ‚Ä¢ {change}")
            print(f"\nTotal: {len(changes_made)} change(s)")
            print("\nUse --no-dry-run to apply normalization.")
        else:
            # Apply normalization
            commit_msg = f"Normalize metadata: {', '.join(changes_made[:3])}"
            if len(changes_made) > 3:
                commit_msg += f" (+{len(changes_made) - 3} more)"

            if self._save_metadata(normalized_metadata, commit_message=commit_msg):
                print(f"\n‚úì Normalized metadata:")
                for change in changes_made:
                    print(f"   ‚Ä¢ {change}")
                if self.use_git:
                    print("‚úì Changes committed to git")
            else:
                print("\n‚ùå Failed to normalize metadata")
                return

        print(f"\n{'='*80}\n")

    def rollback(self, commit_hash: Optional[str] = None, dry_run: bool = True) -> None:
        """
        Roll back changes to custom_scripts.json using git history.

        This function can restore custom_scripts.json to a previous state using git.
        It NEVER modifies script files, only the metadata JSON.

        Args:
            commit_hash: Specific commit hash to roll back to (default: previous commit)
            dry_run: If True, only show what would be rolled back
        """
        print(f"\n{'='*80}")
        print("Rollback Changes")
        print(f"{'='*80}\n")

        if not self.use_git:
            print("‚ùå Git is not available. Cannot rollback using git.")
            print("\nAlternative: Check for backup files:")
            backup_files = sorted(self.scripts_dir.glob("custom_scripts.json.backup_*"))
            if backup_files:
                print(f"Found {len(backup_files)} backup(s):")
                for backup in backup_files[-5:]:  # Show last 5
                    print(f"   ‚Ä¢ {backup.name}")
                print("\nYou can manually restore from a backup file.")
            else:
                print("No backup files found.")
            print(f"\n{'='*80}\n")
            return

        # Get git log
        result = self._run_git(['log', '--oneline', '-20'], capture_output=True, check=False)
        if not result or result.returncode != 0:
            print("‚ùå Could not access git history")
            print(f"\n{'='*80}\n")
            return

        commits = result.stdout.strip().split('\n') if result.stdout.strip() else []

        if not commits:
            print("No git history found.")
            print(f"\n{'='*80}\n")
            return

        # Show recent commits
        print("Recent commits affecting custom_scripts.json:")
        print("-" * 80)
        relevant_commits = []
        for commit_line in commits:
            if 'custom_scripts.json' in commit_line.lower() or len(relevant_commits) < 5:
                commit_hash_val = commit_line.split()[0] if commit_line else None
                commit_msg = ' '.join(commit_line.split()[1:]) if len(commit_line.split()) > 1 else commit_line
                if commit_hash_val:
                    relevant_commits.append((commit_hash_val, commit_msg))
                    print(f"   {commit_hash_val[:8]} - {commit_msg}")

        if not commit_hash:
            # Default to previous commit
            if len(relevant_commits) > 1:
                commit_hash = relevant_commits[1][0]  # Second commit (first is current)
                print(f"\nWill rollback to: {commit_hash[:8]} - {relevant_commits[1][1]}")
            else:
                print("\n‚ùå No previous commit to rollback to")
                print(f"\n{'='*80}\n")
                return
        else:
            # Validate commit hash
            if len(commit_hash) < 7:
                # Try to find matching commit
                matching = [c for c in relevant_commits if c[0].startswith(commit_hash)]
                if matching:
                    commit_hash = matching[0][0]
                    print(f"\nRolling back to: {commit_hash[:8]} - {matching[0][1]}")
                else:
                    print(f"\n‚ùå Commit hash not found: {commit_hash}")
                    print(f"\n{'='*80}\n")
                    return
            else:
                # Full or partial hash provided
                matching = [c for c in relevant_commits if c[0].startswith(commit_hash[:8])]
                if matching:
                    commit_hash = matching[0][0]
                    print(f"\nRolling back to: {commit_hash[:8]} - {matching[0][1]}")
                else:
                    print(f"\n‚ö†Ô∏è  Warning: Commit {commit_hash[:8]} not in recent history")
                    print("Proceeding anyway...")

        if dry_run:
            print("\n[DRY RUN] Would restore custom_scripts.json from commit")
            print("Use --no-dry-run to actually rollback.")
        else:
            # Show current file state
            current_size = self.custom_scripts_json.stat().st_size if self.custom_scripts_json.exists() else 0
            print(f"\nCurrent file size: {current_size:,} bytes")

            # Get file from that commit
            result = self._run_git(['show', f'{commit_hash}:custom_scripts.json'],
                                  capture_output=True, check=False)

            if not result or result.returncode != 0:
                print(f"‚ùå Could not retrieve file from commit {commit_hash[:8]}")
                print(f"\n{'='*80}\n")
                return

            # Validate JSON
            try:
                restored_data = json.loads(result.stdout)
                print(f"Restored file would be: {len(result.stdout):,} bytes")
                print(f"Would contain {len(restored_data)} script entries")
            except json.JSONDecodeError:
                print("‚ö†Ô∏è  Warning: Restored file is not valid JSON")
                response = input("Continue anyway? (y/N): ").strip().lower()
                if response != 'y':
                    print("Rollback cancelled.")
                    print(f"\n{'='*80}\n")
                    return

            # Create backup of current state
            backup_path = self._backup_file(self.custom_scripts_json)
            if backup_path:
                print(f"‚úì Created backup: {backup_path.name}")

            # Restore file
            try:
                with open(self.custom_scripts_json, 'w', encoding='utf-8') as f:
                    f.write(result.stdout)

                # Commit the rollback
                commit_msg = f"Rollback to {commit_hash[:8]}"
                self._run_git(['add', str(self.custom_scripts_json)], check=False)
                self._run_git(['commit', '-m', commit_msg], check=False)

                print(f"\n‚úì Rolled back custom_scripts.json to commit {commit_hash[:8]}")
                if self.use_git:
                    print("‚úì Rollback committed to git")
            except Exception as e:
                print(f"\n‚ùå Error during rollback: {e}")
                # Try to restore from backup
                if backup_path and backup_path.exists():
                    try:
                        shutil.copy2(backup_path, self.custom_scripts_json)
                        print(f"‚úì Restored from backup: {backup_path.name}")
                    except:
                        pass
                return

        print(f"\n{'='*80}\n")

    def show_diff(self, commit_hash: Optional[str] = None,
                  use_color: bool = True) -> None:
        """
        Show colored diff of changes to custom_scripts.json.

        Args:
            commit_hash: Show diff for specific commit (default: show unstaged changes)
            use_color: Use colored output (default: True)
        """
        if not self.use_git:
            print("‚ùå Git is not available. Cannot show diffs.")
            print(f"\n{'='*80}\n")
            return

        print(f"\n{'='*80}")
        if commit_hash:
            print(f"Diff for commit {commit_hash[:8]}")
        else:
            print("Diff (unstaged changes)")
        print(f"{'='*80}\n")

        # Build git diff command
        diff_args = ['diff']
        if use_color:
            diff_args.append('--color=always')
        else:
            diff_args.append('--color=never')

        if commit_hash:
            # Show diff for specific commit
            if len(commit_hash) < 7:
                # Try to find matching commit
                result = self._run_git(['log', '--oneline', '-20'], capture_output=True, check=False)
                if result and result.returncode == 0:
                    commits = result.stdout.strip().split('\n') if result.stdout.strip() else []
                    matching = [c.split()[0] for c in commits if c.split()[0].startswith(commit_hash)]
                    if matching:
                        commit_hash = matching[0]

            # Show diff between commit and its parent
            diff_args.extend([f'{commit_hash}^..{commit_hash}', '--', 'custom_scripts.json'])
        else:
            # Show unstaged changes
            diff_args.append('--')
            diff_args.append('custom_scripts.json')

        result = self._run_git(diff_args, capture_output=True, check=False)

        if result and result.returncode == 0:
            if result.stdout:
                print(result.stdout)
            else:
                print("No changes to show.")
        elif result and result.returncode == 1:
            # Exit code 1 means there are differences (this is normal for git diff)
            if result.stdout:
                print(result.stdout)
            else:
                print("No changes to show.")
        else:
            print("‚ùå Could not retrieve diff")
            if result and result.stderr:
                print(f"Error: {result.stderr}")

        print(f"\n{'='*80}\n")

    def export_summary(self, output_file: Optional[str] = None) -> None:
        """Export a summary report to a file"""
        if output_file is None:
            output_file = self.scripts_dir / "script_summary.txt"

        original_stdout = sys.stdout
        with open(output_file, 'w', encoding='utf-8') as f:
            sys.stdout = f
            self.list_scripts(show_details=True)
            self.validate_and_report()
            sys.stdout = original_stdout

        print(f"Summary exported to: {output_file}")

    def manage_files(self, auto_sync: bool = False, show_status: bool = False) -> None:
        """
        Manage files you add - automatically track new files in metadata.

        This function helps manage files you manually add to the scripts directory
        by automatically adding them to custom_scripts.json.

        Args:
            auto_sync: If True, automatically add all orphaned files to metadata
            show_status: If True, show detailed status of all files
        """
        print(f"\n{'='*80}")
        print("Manage Files")
        print(f"{'='*80}\n")

        issues = self.validate()

        if show_status:
            # Show detailed status
            metadata_list = self.load_metadata()
            script_files = self.get_script_files()

            metadata_files_lower = {m.file.lower() for m in metadata_list}

            print("File Status:")
            print("-" * 80)

            # Files in metadata
            print(f"\n‚úì Tracked in metadata ({len(metadata_list)}):")
            for metadata in sorted(metadata_list, key=lambda x: x.name.lower()):
                file_path = self.scripts_dir / metadata.file
                if file_path.exists():
                    size = file_path.stat().st_size
                    print(f"   ‚Ä¢ {metadata.file} ({size:,} bytes) - {metadata.name}")
                else:
                    print(f"   ‚Ä¢ {metadata.file} - ‚úó FILE MISSING")

            # Orphaned files
            orphaned = [f for f in script_files if f.name.lower() not in metadata_files_lower]
            if orphaned:
                print(f"\n‚ö†Ô∏è  Not tracked ({len(orphaned)}):")
                for script_file in sorted(orphaned, key=lambda x: x.name.lower()):
                    size = script_file.stat().st_size
                    print(f"   ‚Ä¢ {script_file.name} ({size:,} bytes)")
            else:
                print(f"\n‚úì All files are tracked")

            # Missing files
            if issues['missing_files']:
                print(f"\n‚ùå Missing files ({len(issues['missing_files'])}):")
                for file in issues['missing_files']:
                    print(f"   ‚Ä¢ {file}")

        # Auto-sync if requested
        if auto_sync:
            if issues['orphaned_files']:
                print(f"\n{'='*80}")
                print(f"Auto-syncing {len(issues['orphaned_files'])} file(s)...")
                print(f"{'='*80}\n")
                self.sync_metadata(dry_run=False, files_to_manage=issues['orphaned_files'])
            else:
                print("\n‚úì All files are already tracked in metadata")
        elif issues['orphaned_files']:
            print(f"\nFound {len(issues['orphaned_files'])} file(s) not in metadata:")
            for file in issues['orphaned_files']:
                print(f"   ‚Ä¢ {file}")
            print("\nRun with --auto-sync to automatically add them to metadata")

        print(f"\n{'='*80}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Manage Draw Things scripts',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s list                    # List all scripts
  %(prog)s list --details          # List with detailed information
  %(prog)s validate                # Validate scripts and report issues
  %(prog)s sync                    # Show what would be synced (dry run)
  %(prog)s sync --no-dry-run       # Actually sync metadata
  %(prog)s manage --status         # Show status of all files
  %(prog)s manage --auto-sync      # Auto-track files you add
  %(prog)s normalize               # Normalize metadata (fix case, sort)
  %(prog)s normalize --no-dry-run  # Apply normalization
  %(prog)s rollback                # Show rollback options (dry run)
  %(prog)s rollback --no-dry-run  # Rollback to previous commit
  %(prog)s rollback --commit HASH  # Rollback to specific commit
  %(prog)s diff                    # Show colored diff of unstaged changes
  %(prog)s diff --commit HASH      # Show diff for specific commit
  %(prog)s export                  # Export summary report
        """
    )

    parser.add_argument(
        '--scripts-dir',
        help='Path to Scripts directory (default: auto-detect)',
        default=None
    )

    subparsers = parser.add_subparsers(dest='command', help='Command to run')

    # List command
    list_parser = subparsers.add_parser('list', help='List all scripts')
    list_parser.add_argument('--details', action='store_true', help='Show detailed information')

    # Validate command
    subparsers.add_parser('validate', help='Validate scripts and report issues')

    # Sync command
    sync_parser = subparsers.add_parser('sync', help='Sync metadata (add orphaned files)')
    sync_parser.add_argument('--no-dry-run', action='store_true',
                           help='Actually apply changes (default is dry run)')
    sync_parser.add_argument('--files', nargs='+', metavar='FILE',
                           help='Specific files to add to metadata (default: all orphaned files)')

    # Git status command
    subparsers.add_parser('git-status', help='Show git status of scripts directory')

    # Manage command
    manage_parser = subparsers.add_parser('manage', help='Manage files you add (auto-track in metadata)')
    manage_parser.add_argument('--auto-sync', action='store_true',
                              help='Automatically add orphaned files to metadata')
    manage_parser.add_argument('--status', action='store_true',
                              help='Show detailed status of all files')

    # Normalize command
    normalize_parser = subparsers.add_parser('normalize', help='Normalize metadata (fix case, sort, standardize)')
    normalize_parser.add_argument('--no-dry-run', action='store_true',
                                 help='Actually apply normalization (default is dry run)')
    normalize_parser.add_argument('--no-fix-case', action='store_true',
                                 help='Skip fixing case mismatches')
    normalize_parser.add_argument('--no-sort', action='store_true',
                                 help='Skip sorting entries')

    # Rollback command
    rollback_parser = subparsers.add_parser('rollback', help='Rollback changes to custom_scripts.json')
    rollback_parser.add_argument('--commit', metavar='HASH',
                                help='Specific commit hash to rollback to (default: previous commit)')
    rollback_parser.add_argument('--no-dry-run', action='store_true',
                                help='Actually rollback (default is dry run)')

    # Diff command
    diff_parser = subparsers.add_parser('diff', help='Show colored diff of changes')
    diff_parser.add_argument('--commit', metavar='HASH',
                            help='Show diff for specific commit (default: unstaged changes)')
    diff_parser.add_argument('--no-color', action='store_true',
                            help='Disable colored output')

    # Export command
    export_parser = subparsers.add_parser('export', help='Export summary report')
    export_parser.add_argument('--output', help='Output file path')

    args = parser.parse_args()

    manager = ScriptManager(scripts_dir=args.scripts_dir)

    if args.command == 'list':
        manager.list_scripts(show_details=args.details)
    elif args.command == 'validate':
        is_valid = manager.validate_and_report()
        sys.exit(0 if is_valid else 1)
    elif args.command == 'sync':
        manager.sync_metadata(dry_run=not args.no_dry_run, files_to_manage=getattr(args, 'files', None))
    elif args.command == 'git-status':
        status = manager.git_status()
        print(f"\n{'='*80}")
        print("Git Status")
        print(f"{'='*80}\n")
        if 'error' in status:
            print(f"‚ùå Error: {status['error'][0]}")
        elif status['clean']:
            print("‚úì Working directory is clean")
        else:
            if status['modified']:
                print(f"Modified files ({len(status['modified'])}):")
                for f in status['modified']:
                    print(f"  ‚Ä¢ {f}")
            if status['staged']:
                print(f"\nStaged files ({len(status['staged'])}):")
                for f in status['staged']:
                    print(f"  ‚Ä¢ {f}")
            if status['untracked']:
                print(f"\nUntracked files ({len(status['untracked'])}):")
                for f in status['untracked']:
                    print(f"  ‚Ä¢ {f}")
        print(f"\n{'='*80}\n")
    elif args.command == 'manage':
        manager.manage_files(
            auto_sync=getattr(args, 'auto_sync', False),
            show_status=getattr(args, 'status', False)
        )
    elif args.command == 'normalize':
        manager.normalize(
            dry_run=not getattr(args, 'no_dry_run', False),
            fix_case=not getattr(args, 'no_fix_case', False),
            normalize_json=True,
            sort_entries=not getattr(args, 'no_sort', False)
        )
    elif args.command == 'rollback':
        manager.rollback(
            commit_hash=getattr(args, 'commit', None),
            dry_run=not getattr(args, 'no_dry_run', False)
        )
    elif args.command == 'diff':
        manager.show_diff(
            commit_hash=getattr(args, 'commit', None),
            use_color=not getattr(args, 'no_color', False)
        )
    elif args.command == 'export':
        manager.export_summary(output_file=args.output)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

